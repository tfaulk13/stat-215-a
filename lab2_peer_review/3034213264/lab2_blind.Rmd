---
title: "Lab 2 - Linguistic Survey Stat 215A, Fall 2018"
author: "3034213264"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: bibliography.bib
header-includes:
   - \usepackage{float}
   - \usepackage{gensymb}
output: 
  pdf_document:
    citation_package: natbib
    number_sections: true
---

\setlength{\abovedisplayskip}{-3mm}
\setlength{\belowdisplayskip}{1mm}

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE)

# load in useful packages
library(tidyverse)
library(R.utils)
library(lubridate)
library(Hmisc)
library(zipcode)
library(stringr)
library(ggpubr)
library(maps)
library(caret)
library(irlba)
library(GGally)
library(Rtsne)
library(NMF)
library(leaflet)
library(kernlab)
library(maptools)
library(dendextend)
library(factoextra)
library(NbClust)
library(NNLM)
library(knitr)
library(nnet)
library(grid)

# load in all utility functions from R/ folder
sourceDirectory("./R/", modifiedOnly = F, recursive = F) # useful functions
```

# Kernel density plots and smoothing

```{r load-redwood-data}

# load the dates data
dates_orig <- loadDatesData(path = "data/")
# clean the dates data
dates <- cleanDatesData(dates_orig)

# load the mote location data
mote_loc <- loadMoteLocationData(path = "data/")

# load the redwood sensor data
redwood_net_orig <- loadRedwoodData(path = "data/", source = "net")
redwood_log_orig <- loadRedwoodData(path = "data/", source = "log")
# clean the redwood sensor data 
redwood_net <- cleanRedwoodData(redwood_net_orig)
redwood_log <- cleanRedwoodData(redwood_log_orig)
# merge redwood data with dates and mote locations
redwood_all <- mergeRedwoodData(date_df = dates, mote_loc_df = mote_loc,
                                redwood_net_df = redwood_net,
                                redwood_log_df = redwood_log)
# remove outliers
redwood <- rmOutliersRedwoodData(redwood_all)$redwood_rm_out
```

For this section, we use the cleaned redwood dataset from the previous lab. In Figure \ref{fig:kernels}, we experiment with kernel density estimates of the redwood temperature distribution using various kernels and bandwidths. 

```{r temp-kernels, fig.width = 7, fig.height = 5, fig.align="center", fig.cap = "\\label{fig:kernels} Distribution of temperature measurements for the entire redwood study. The kernel density subplots from left to right increase in bandwidth, and each row corresponds to a different kernel function.", fig.pos = "H"}

# redwood temperature data
temp <- redwood$temp

# types of kernels to try
kernels <- c("gaussian", "epanechnikov", "rectangular")

# plot titles
plt_titles <- capitalize(kernels)
                
# bandwidths
bws <- c(1e-2, 1e-1, 1, 10)

# initialize list of plots
plt_ls <- list()

# initialize base plot
base_plt <- ggplot(data.frame(temp = temp)) + aes(x = temp)

# make plot for various kernels and bandwidths
plt_idx <- 1; title_idx <- 1
for (kernel in kernels) {
  for (bw in bws) {
    plt_ls[[plt_idx]] <- base_plt + 
      stat_density(bw = bw, kernel = kernel, geom = "line", color = "red") +
      labs(x = "Temperature", y = "Density",
           title = paste0(plt_titles[title_idx], 
                          "\nBandwidth = ", bw)) +
      myGGplotTheme(title_size = rel(.7), 
                    axis_title_size = rel(.5), axis_text_size = rel(.45)) +
      theme(axis.title = element_text(face = "plain"),
            axis.line = element_line(size = .5, color = "black"))
    plt_idx <- plt_idx + 1
  }
  title_idx <- title_idx + 1
}

ggarrange(plotlist = plt_ls, ncol = length(bws), nrow = length(kernels))


```


We clearly observe a trade-off between bias and variance, depending on the choice of bandwidth. As we move through the subplots from left to right in Figure \ref{fig:kernels} (i.e. as the bandwidth increases), the kernel density estimate becomes much smoother, which corresponds to lower variance but higher bias. As bandwidth decreases, the density estimate fluctuates greatly and eventually overfits to spurious artifacts and noise in the data. In the case when the bandwidth is too small, bias is low while variance is high. We point out that a bandwidth of around 1 seems to be a reasonable compromise between the two extremes. 

Though the bandwidth appears to heavily influence the density estimate, the choice of kernel (e.g. Gaussian, Epanechnikov, Rectangular) does not appear to impact the densities as much, particularly when the bandwidth is small. When the bandwidth is large however, the density estimates increasingly resemble the kernel function. For example, the rectangular kernel density estimate when the bandwidth was 100 is becoming more like the rectangular kernel function. Additionally, when the bandwidth was 1, we observe kinks in the rectangular kernel density estimate while the other density estimates using differentiable kernels (e.g. the Gaussian and Epanechnikov kernels) gave smooth estimates. Overall, we conclude that the density estimates are relatively robust to the choice of kernel, but the choice of bandwidth is heavily influential in the bias-variance trade-off.

We next study the relationship between temperature and humidity at noon throughout the redwood study using loess smoothing with different bandwidths and polynomial degrees. 

```{r loess-plt, fig.width = 7, fig.height = 5, fig.align="center", fig.cap = "\\label{fig:loess} Relationship between temperature and humidity at noon for the entire redwood study. The amount of smoothing used in the loess increases as we move from left to right. The degree of the polynomial used in the loess increases as we move from top to bottom.", fig.pos = "H"}

# get all temperature and humidity measurements for all nodes at noon
redwood_time_of_day <- redwood %>%
  filter(time == " 12:00:00") %>%
  select(temp, humidity)

spans <- c(.9, 1, 2, 10)
degrees <- list(y ~ poly(x, 1), y ~ poly(x, 2), y ~ poly(x, 3))

# initialize a base plot for the given degree of the loess smoother
base_plt <- ggplot(redwood_time_of_day) +
  aes(x = temp, y = humidity) +
  geom_point(size = .1, alpha = .25) +
  myGGplotTheme() 

# make plot for various spans and degrees
plt_ls <- list(); plt_idx <- 1
for (i in 1:3) {
  for (span in spans) {
  plt_ls[[plt_idx]] <- base_plt + 
    geom_smooth(method = "loess", se = F, span = span,
                formula = degrees[[i]], color = "red") +
    labs(x = "Temperature", y = "Humidity",
         title = paste0("Degree = ", i, 
                        "\nSpan = ", span)) +
    myGGplotTheme(title_size = rel(.7), 
                  axis_title_size = rel(.5), axis_text_size = rel(.45)) +
    theme(axis.title = element_text(face = "plain"),
          axis.line = element_line(size = .5, color = "black"))
  plt_idx <- plt_idx + 1
  }
}

ggarrange(plotlist = plt_ls, ncol = length(spans), nrow = length(degrees))

```

As in the previous investigation, we see a trade-off between bias and variance. As the degree increases (i.e. as we move down the columns in Figure \ref{fig:loess}), the fitted loess is more flexible, and variance increases. As the span increases (i.e. as we move from left to right in Figure \ref{fig:loess}), the fitted loess becomes smoother so that the bias increases while variance decreases. We point out that in most cases, the loess has a negative slope, suggesting a negative relationship between humidity and temperature. However, when the degree is 2 or 3 and the span is small enough (e.g. 0.9 or 1), the loess overfits to a group of outlying points with abnormally low humidity levels. This disrupts the negative pattern that we observe in all other subplots, illustrating that overfitting is a concern when selecting loess parameters. On the other hand, underfitting is also a valid concern. After fitting loess with different degrees and spans, we conclude that a degree 2 polynomial with span 10 appropriately fits the data, and it suggests that there is a negative, slightly nonlinear relationship between temperature and humidity.


# Linguistic Data

## Introduction

The study of language variation is a valuable field of linguistics, which can provide unique insights into historical, social, and geographical factors in society. For instance, shared linguistic traits between two groups of people may suggest interpersonal contact at some point in history. Language variation among social classes may signal social identity and prominence, and it is well known that geographical distances can influence how one speaks. Nevertheless, there are many challenges in studying linguistic variation, a gradual and slow process with many irregularities. In this report, we focus upon the study of dialects, or dialectology, and the measurement of dialect differences, or dialectometry. We consider a Dialect Survey conducted by Bert Vaux \citep{vaux2003survey}, and we aim to (1) understand whether there are clusters of people with similar survey responses and (2) determine whether these clusters correspond to geographical locations. Before proceeding though, we cautiously note that language is known to depend on sex, age, occupation, social class, as well as geography, so while we study the relationship between geography and language here, further investigation is required to explore more complex relationships between language and other covariates.

## The Data

In this study, we analyze data from the Dialect Survey conducted by Bert Vaux \citep{vaux2003survey}, and in particular, we focus on the survey questions regarding lexical differences, rather than phonetic differences. The overall survey consisted of $p = 67$ questions of interest and $n = 47,471$ respondents from across the United States with their geographical location encoded by city, state, and ZIP code.

### Data quality and cleaning

As with any survey data, there are always misspellings and typos of cities, states, and ZIP code. While we are aware of this issue, it does not seem to be a large obstacle for our downstream analysis, so we deal with this issue as it comes up. The more pressing issues are missing locations and unanswered survey questions. 

We found that there were three entries with missing state identifications. Two of the three listed cities which appeared to be outside of the US, and the third listed the city to be "nowhere." We thus deleted these three observations. There were also missing latitude and longitude values for 1020 observations. To reduce this number, we merged our given survey data with the zipcode data from \texttt{library(zipcode)}. However, in this process of merging, we noticed that many ZIP codes in the survey data had only four digits rather than the usual five digits. We believe that a leading 0 in the ZIP code was dropped when the ZIP code was converted from a string to numeric, so we padded the four digit ZIP codes with a leading 0. Then we merged the survey data with the zipcode data by ZIP and by state. We included state as a precaution in case the ZIP code was inputted incorrectly. After this merge, there were 648 observations with missing latitude and longitude data. Careful investigation into the ZIP codes with missing latitudes and longitudes reveals that several of these ZIP codes appeared with high frequencies (e.g. the ZIP code 95411 was entered 86 times but did not have a corresponding location). We speculate that there may have been a change in the ZIP code recently, so removing these observations may introduce some location bias. However, as there is not much we can do, we were forced to omit the 648 observations with missing latitude and longitude data.

Finally, because a non-negligible number of people answered only a few questions, we decided to omit those who left more than ten questions unanswered. We chose the threshold of ten after looking at the distribution of the number of unanswered questions per person and saw that ten was a reasonable balance between removing too many observations and keeping too many non-informative observations. After this pre-processing, we were left with $n = 45,332$ samples.

With regards to the data quality, we note that the sampling is not necessarily representative of the US population. It seems to favor responses from the northern US. For instance, 2,506 responses were from Texas (population 28.3 million) while 2,575 responses were from Pennsylvania (population 12.8 million). Additionally, there were very few responses from the western US, aside from California. We keep this sampling bias in mind throughout the analysis. 

```{r load-ling-data}

# read in linguistics data
ling_original <- loadLingData(path = "data/")

# load in question data
load("./data/question_data.Rdata")

# clean linguistics data
ling <- cleanLingData(ling_original)

```


### Exploratory Data Analysis

In our exploration of the linguistics data, we choose to compare survey questions 65 and 66. That is, "What do you call the insect that flies around in the summer and has a rear section that glows in the dark?" and "What do you call the miniature lobster that one finds in lakes and streams for example (a crustacean of the family Astacidae)?" For both questions, there were three answers which were overwhelmingly popular (encapsulating over 90\% of the surveyors in each case). We henceforth only consider the three most popular answer choices for each question to enhance clarity.

\textit{Remark}: Throughout this report, we include Hawaiians and Alaskans in all statistical analyses, but we refrain from plotting their responses on the maps to avoid clutter.

```{r exploratory-relationships, fig.width = 6, fig.height = 5.5, fig.align="center", fig.cap = "\\label{fig:explore} Responses to questions 65 and 66 according to geographical location.", fig.pos = "H", eval = T}

# let's look at the relationship between Q# and Q#
qq_plt_df <- ling %>%
  # select two questions to compare
  select(ID, CITY, STATE, ZIP, ZIP_str, lat, long, Q065, Q066) %>%
  # only consider those who answer the 3 most popular answer choices
  filter(Q065 %in% 1:3, Q066 %in% c(1, 2, 5)) 

# get answers to the two questions
answers65 <- all.ans[['65']]$ans[1:3]
answers66 <- all.ans[['66']]$ans[c(1, 2, 5)]

# change numbered answers to the actual responses/answers
qq_plt_df <- qq_plt_df %>%
  mutate(Q065 = as.factor(Q065),
         Q066 = as.factor(Q066))
levels(qq_plt_df$Q065) <- answers65
levels(qq_plt_df$Q065)[3] <- "either "
levels(qq_plt_df$Q066) <- answers66
qq_plt_df$Q066 <- factor(qq_plt_df$Q066, levels = c("crawdad ", 
                                                    "crayfish ", "crawfish "))

# initialize list of map plts
plt_ls <- list()
plt_df <- qq_plt_df %>% filter(long > -125) # don't plot HI or AK

# plot responses to Q65 and Q66
plt_ls[[1]] <- plotMapClusters(loc = plt_df %>% select(lat, long), 
                               y = plt_df$Q065, ylab = "Answer",
                               point.size = .15, alpha = .75,
                               viridis_color = "viridis", 
                               title = "What do you call the insect that flies around in the summer and has a rear section that glows in the dark?") +
  theme(title = element_text(size = rel(.7))) +
  guides(colour = guide_legend(override.aes = list(size = 2)))
plt_ls[[2]] <- plotMapClusters(loc = plt_df %>% select(lat, long), 
                               y = plt_df$Q066, ylab = "Answer",
                               point.size = .15, alpha = .75,
                               viridis_color = "viridis", 
                               title = "What do you call the miniature lobster that one finds in lakes and streams for example?") +
  theme(title = element_text(size = rel(.7))) + 
  guides(colour = guide_legend(override.aes = list(size = 2)))

ggarrange(plotlist = plt_ls, ncol = 1, nrow = 2)

```

From Figure \ref{fig:explore}, we observe that responses tend to cluster around certain geographical locations. In the top panel of Figure \ref{fig:explore}, lightning bug is clustered around the mid west while firefly is used more the in the west. The bottom panel of Figure \ref{fig:explore} shows a slightly different pattern with crawdad being used frequently in the mid west, crawfish in the south, and crayfish in the north/northeast.

```{r confusion, results = 'asis'}
# try logistic regression or svm?

# confusion matrix
confusion <- table(qq_plt_df$Q065, qq_plt_df$Q066) / nrow(qq_plt_df) * 100
kable(confusion, col.names = colnames(confusion), align = "rccc", digits = 1,
      caption = "Confusion matrix for survey questions 65 and 66 (in %)")

```

Looking at the confusion matrix in Table 1, we see that though people who answered crawdad are more likely to answer lightning bug than the other two choices, the responses appear fairly evenly distributed across the confusion matrix. Consequently, if we were to use the crustacean answer to predict the insect answer, the optimal predictor (measured via classification accuracy) would be as follows: if x = crawdad, then predict lightning bug; if x = crayfish, then predict either; and if x = crawfish, then predict either. (This can be seen by finding the largest percentage in each column of Table 1.) Under this prediction rule, the \textit{maximum} classification accuracy we can obtain is $11.6 + 12.1 + 17.7 = 41.4\%$. Since we used all of the data to obtain this estimate, $41.1\%$ is most likely an overestimate of the true population classification accuracy. Moreover, $41.1\%$ is only slightly higher than random guessing ($33\%$). If we were to use the insect answer to predict the crustacean answer, we would see a similar story. This suggests that while the two questions appear to define geographical regions, the questions are not highly predictive of one another.

If one were to add another question into the mix, then the predictive value can increase. However, after trying many different combinations, it was difficult to achieve greater than $50\%$ test classification accuracy using a simple multinomial regression to predict the insect response from the crustacean response and one other survey response.

```{r log-reg, eval = F}

set.seed(123)

# get answers to Q65, Q66, Q73
X <- ling %>% 
  select(Q065, Q066, Q050) %>% 
  filter(Q065 %in% 1:3, Q066 %in% c(1, 2, 5), Q050 %in% c(1, 4, 7, 9)) %>%
  mutate_all(as.factor)

# randomly split into training and test set
ind <- sample(1:nrow(X), .75*nrow(X), replace = F)
train <- X[ind,]
test <- X[-ind,]

# fit multinomial regression and predict the class
log_fit <- multinom(Q066 ~ Q065 + Q050, data = train)
pred <- predict(log_fit, test[,-2], type = "class")
sum(pred == test$Q066) / nrow(test)

```


## Dimension reduction methods

To gain a better understanding of dominant patterns in the high-dimensional linguistics data, we experiment with several dimension reduction techniques, the first being PCA. However, it does not make much sense to perform PCA on the categorical linguistics data because the survey responses are unordered. We thus transform the categorical survey responses to binary indicator variables and apply the dimension reduction techniques to this binary dataset.

```{r binary-encoding}

# binary encoding of questions only
quest <- ling %>% select(starts_with("Q")) # only get the question data
rownames(quest) <- ling$ID
quest_binary <- binary_encoding(df = quest)

# append id/labels/long/lat to binary questions data
ling_id <- ling %>% select(-starts_with("Q"))
ling_binary <- cbind(ling_id, quest_binary)

```

```{r pca-explore1, fig.width = 5, fig.height = 2.5, fig.align="center", fig.cap = "\\label{fig:pca_explore1} Here, we show the first two PCs, colored by longitude, after applying PCA to the uncentered binary linguistics data.", fig.pos = "H"}

# perform PCA with uncentered data
pca_uncentered <- plotPCA(dat = quest_binary, npcs = 2, 
                          y = ling$long, ylab = "Longitude", 
                          title = "PCA on Uncentered Binary Linguistics Data",
                          var = T, is.discrete = F,
                          center.pca = F, scale.pca = F,
                          subsample = .15, alpha = .5, point.size = .05,
                          show.plt = F)

# plot pca
pca_uncentered$pca_plt + theme(axis.text = element_text(size = rel(.5)),
                               axis.title = element_text(size = rel(1.25)),
                               legend.text = element_text(size = rel(.5)),
                               legend.title = element_text(size = rel(1)),
                               title = element_text(size = rel(.6)))

# top features corresponding to pc1
loadings_df <- data.frame(q = colnames(quest_binary), 
                          loadings = abs(pca_uncentered$pca_loadings[,1])) %>%
  arrange(-loadings)
top_q <- loadings_df$q[1:5]

```


In Figure \ref{fig:pca_explore1}, we apply PCA to the uncentered binary linguistics data. From Figure \ref{fig:pca_explore1}, we see that the first PC explains a lot of the variation in the data ($46.1 \%$ as displayed in the x-axis title) while the second PC explains only $2\%$ of the variation in the data. The scree plot is not displayed, but it is clear from the first 2 PCs that there is an elbow in the scree plot after the first PC. This is an interesting observation, but further investigation reveals that the first PC is not associated with geographical location in any way. Rather, the variation explained by PC1 corresponds to the variation within questions, which had a single dominant response. That is, the top 2 features with the largest PC1 loadings (in magnitude) corresponded to Q63, answer choice 1 and Q67, answer choice 1 - both of which received over $90\%$ of the votes for their respective question. Since the variation within questions is not of interest to us, we next try PCA on the centered binary linguistics data, as shown in Figure \ref{fig:pca_explore2}.

```{r pca-explore2, fig.width = 5.5, fig.height = 4, fig.align="center", fig.cap = "\\label{fig:pca_explore2} PCA on the centered binary linguistics data. The lower triangle of PC plots are colored by longitude while the upper triangle of PC plots are colored by latitude", fig.pos = "H"}

# apply PCA with centered data
pca_centered <- plotPCA(dat = quest_binary, npcs = 3, 
                        y = ling$long, y_additional = ling$lat,
                        var = T, center.pca = T, scale.pca = F,
                        subsample = .15, alpha = .001, point.size = .001,
                        show.plt = F)
# plot pca
pca_centered$pca_plt

# magnitude of pc1 loadings
pc1_loadings <- data.frame(q = colnames(quest_binary),
                           loading = abs(pca_centered$pca_loadings[,1])) %>%
  arrange(-loading)
# pc1_loadings[1:10,] # -> top questions: Q73, Q105, Q80, Q106, Q56

# magnitude of pc1 loadings
pc2_loadings <- data.frame(q = colnames(quest_binary),
                           loading = abs(pca_centered$pca_loadings[,2])) %>%
  arrange(-loading)
# pc2_loadings[1:10,] # -> top questions: Q76, Q103, Q50

```

Pair plots for the first 3 PCs are shown in Figure \ref{fig:pca_explore2}. The lower triangle of PC plots are colored by longitude with yellow corresponding to the northern US, orange being the middle US, and purple the southern US. The upper triangle of PC plots are colored by latitude with yellow being the eastern US, green the central US, and blue the western US. Our first observation is that the proportions of variance explained (shown in the parentheses in the PC titles) are very small, indicating that if one were to simply take the first three PCs and ignore the other the PCs, we would be losing a lot of variation in the data. However, even though these PCs capture a small portion of the variation in the data, they appear to be associated with geography. This is in stark contrast to PCA on the uncentered data. Specifically, in the first PC in Figure \ref{fig:pca_explore2}, the northerners in yellow are fairly differentiated from the rest of the data. By looking at the top three largest PC1 loadings (in magnitude), we are led to believe that the sneakers (Q73), soda (Q105), and sunshower (Q80) questions help to differentiate the north/northeast from the rest of the US. Then in the PC2 vs PC3 plot, there seems to be a slight gradient from yellow to green to blue, suggesting some association with the eastern vs. western dialects. The important features for PC2 were the catty-corner (Q76), water fountain (Q103), and the plural you (Q50) questions. We also try PCA on the scaled dataset, but we believe scaling is unnecessary, particularly because the features were all measured using the same scale.

While PCA has enabled us to gain some intuition into the data, PCA is limited in the sense that it is a linear projection method. This motivates us to try another dimension reduction method - a nonlinear method - to see if we can explore more complex relationships. In this light, we try a popular nonlinear dimension reduction method called t-Distributed Stochastic Neighbor Embedding (t-SNE) \citep{maaten2008visualizing}, but to apply t-SNE, we must reduce the size of the data for computational feasibility. This is the computational price we must pay for nonlinearity. (Note that PCA is computationally advantageous for large datasets because one only needs to compute the first few eigenvectors of interest, which can be done very efficiently. This illustrates a trade-off between computation and model complexity.)

We choose to reduce the size of the data by aggregating the responses by county instead of the 1 degree latitude-longitude squares. We believe that there is a greater sense of community and cohesiveness within a county, rather than a 1 degree latitude-longitude square that ignores state borders and community lines. To aggregate by county, we first mapped each person's latitude-longitude to a particular county, then summed the binary data for each county, and finally divided each county's data by the number of observations in that county. After binning observations in this way, we ended up with $n = 2372$ counties. We use this binary county data for the remainder of the report to speed up computation.

```{r county-data}

# bin linguistics data by county
ling_loc <- cleanLingLocationData(ling)

```

```{r tsne-explore, fig.width = 8, fig.height = 2.75, fig.align="center", fig.cap = "\\label{fig:tsne_explore} t-SNE on the centered county-level binary linguistics data. Each point represents a county, colored by longitude in (A) and by latitude in (B).", fig.pos = "H"}

# remove duplicates in order to perform for tsne
loc_binary_distinct <- ling_loc %>% select(starts_with("Q")) %>% distinct()
loc <- ling_loc %>% select(county, state, lat, long)
loc_unique <- loc[!duplicated(ling_loc %>% select(starts_with("Q"))),]

# try tsne (nonlinear dimension reduction)
tsne <- Rtsne(loc_binary_distinct, dims = 2, verbose = F, max_iter = 500)

# initialize plot list for tsne
plt_ls <- list() 
# tsne plot df
tsne_plt_df <- data.frame(tsne$Y, loc_unique) %>%
  filter(long > -125) # don't plot HI and AK

# color tsne plot by latitude
plt_ls[[1]] <- ggplot(tsne_plt_df) +
  aes(x = X1, y = X2, color = long) +
  geom_point(size = .5, alpha = .5) +
  myGGplotTheme() +
  scale_color_viridis_c(option = "C") +
  labs(color = "Longitude", x = "t-SNE Component 1", y = "t-SNE Component 2") +
  theme(axis.text = element_text(size = rel(.85)),
        legend.text = element_text(size = rel(.85)))
# color tsne plot by longitude
plt_ls[[2]] <- ggplot(tsne_plt_df) +
  aes(x = X1, y = X2, color = lat) +
  geom_point(size = .5, alpha = .5) +
  myGGplotTheme() +
  scale_color_viridis_c(option = "D") +
  labs(color = "Latitude", x = "t-SNE Component 1", y = "t-SNE Component 2") +
  theme(axis.text = element_text(size = rel(.85)),
        legend.text = element_text(size = rel(.85)))

ggarrange(plotlist = plt_ls, ncol = 2, nrow = 1, labels = "AUTO")
```

Similar to PCA, t-SNE appears to do a respectable job at separating the northern US from the rest of the US (i.e. clustering the yellows in Figure \ref{fig:tsne_explore}A). There is also some differentiation between the eastern US and western US as we see the yellows separate from the dark blues in Figure \ref{fig:tsne_explore}B. It is difficult though to visually compare the t-SNE results with PCA, so we prepare maps in Figure \ref{fig:tsne_explore2}, where each county is colored by the weight of the PCs and t-SNE components. From these maps in Figure \ref{fig:tsne_explore2}, it looks like PC1 and t-SNE component 2 are capturing the same geographical patterns and similarly for PC2 and t-SNE component 1. Because PCA and t-SNE are similar, we prefer PCA over t-SNE since PCA is by far more interpretable.

```{r tsne-explore2, fig.width = 8, fig.height = 4, fig.align="center", fig.cap = "\\label{fig:tsne_explore2} Here, we color each county according to (A) the weights of the first t-SNE component, (B) the second t-SNE component, (C) the first PC, and (D) the second PC."}
# initialize plot list
plt_ls <- list()

# plot tsne weights on county map (component 1)
plt_ls[[1]] <- plotCounties(county_df = tsne_plt_df %>% select(county, state),
                            y = tsne_plt_df$X1, viridis_color = "plasma", 
                            ylab = "", title = "t-SNE Component 1",
                            county.line.color = NA) +
  theme(legend.key.size = unit(.4, "cm"),
        legend.text = element_text(size = rel(.5)),
        plot.title = element_text(hjust = 0.5))
# plot tsne weights on county map (component 2)
plt_ls[[2]] <- plotCounties(county_df = tsne_plt_df %>% select(county, state),
                            y = tsne_plt_df$X2, title = "t-SNE Component 2",
                            viridis_color = "viridis", ylab = "",
                            county.line.color = NA) +
  theme(legend.key.size = unit(.4, "cm"),
        legend.text = element_text(size = rel(.5)),
        plot.title = element_text(hjust = 0.5))

# perform PCA on county level data
ling_loc_quest <- ling_loc %>% select(starts_with("Q"))
pca_county <- prcomp(x = ling_loc_quest, center = T, scale = F)

# plot PC1 weights on county map
plt_ls[[3]] <- plotCounties(county_df = ling_loc %>% select(county, state),
                            y = -pca_county$x[,1], viridis_color = "plasma",
                            ylab = "", title = "Principal Component 1",
                            county.line.color = NA) +
  theme(legend.key.size = unit(.4, "cm"),
        legend.text = element_text(size = rel(.5)),
        plot.title = element_text(hjust = 0.5))

# plot PC2 weights on county map
plt_ls[[4]] <- plotCounties(county_df = ling_loc %>% select(county, state),
                            y = pca_county$x[,2], viridis_color = "viridis",
                            ylab = "", title = "Principal Component 2",
                            county.line.color = NA) +
  theme(legend.key.size = unit(.4, "cm"),
        legend.text = element_text(size = rel(.5)),
        plot.title = element_text(hjust = 0.5))

ggarrange(plotlist = plt_ls, ncol = 2, nrow = 2, labels = "AUTO")
```

## Clustering

While we have seen PCA capturing some geographical patterns from the survey data, we next try to make these patterns more concrete by clustering the county-level survey data. We begin by trying k-means. Since k-means is known to perform poorly in high dimensions, we choose to apply k-means to the first 79 PCs, which explained $75\%$ of the variation in the data. To select $k$, we looked at the within sum of squares and the silhouette information, as shown in Figure \ref{fig:kmeans_choose_k}. The large peak in the silhouette plot at $k = 2$ is highly suggestive, and since there is not clear elbow in the within sum of squares plot that contradicts choosing $k = 2$, we decide to go with $k = 2$. The k-means clusters are depicted in Figure \ref{fig:kmeans}, and it shows that the two clusters correspond to the northern and southern US with a few exceptions. We will examine the stability of these k-means clusters in the next section.

```{r kmeans-choose-k, fig.width = 5.5, fig.height = 2.5, fig.align="center", fig.cap = "\\label{fig:kmeans_choose_k} Metrics for choosing the number of clusters k in k-means. From the within sum of squares and the silhouette information, it appears that k = 2 is an appropriate choice of k for the binary county-level linguistics data.", fig.pos = "H"}

# do k means
top_pcs <- pca_county$x[,1:79]

# choose k via elbow method
elbow <- fviz_nbclust(x = top_pcs, FUNcluster = kmeans, method = "wss") +
  labs(subtitle = "Elbow Method") +
  myGGplotTheme(axis_text_size = rel(.55),
                axis_title_size = rel(.75),
                title_size = rel(.8),
                plot.subtitle = element_text(size = rel(.75)))

# choose k via silhouette method
sil <- fviz_nbclust(x = top_pcs, FUNcluster = kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette Method") +
  myGGplotTheme(axis_text_size = rel(.55),
              axis_title_size = rel(.75),
              title_size = rel(.8),
              plot.subtitle = element_text(size = rel(.75)))

grid.arrange(elbow, sil, nrow = 1, ncol = 2)

```


```{r kmeans, fig.width = 8, fig.height = 3, fig.align="center", fig.cap = "\\label{fig:kmeans} Clusters given by k-means using k = 2. The left panel shows the clusters on the US map while the right shows the clusters on the PC plot. Note that the grey counties do not have any survey responses."}

# from the above diagnostics, we choose k = 2
kmeans_ans <- kmeans(x = top_pcs,
                     centers = 2, nstart = 20)
kmeans_clust <- as.factor(kmeans_ans$cluster)
kmeans_clust_centers <- kmeans_ans$centers

# plot clusters on US mpa
counties_plt <- plotCounties(county_df = ling_loc %>% select(county, state), 
                             y = kmeans_clust, ylab = "", 
                             title = "K Means", show.plt = F, 
                             viridis_color = "viridis", 
                             county.line.color = NA) +
  guides(fill = F) +
  myGGplotFill(discrete = T, na.value = "lightgrey") +
  theme(plot.title = element_text(face = "bold", size = rel(1.5)))

# plot clusters on PC plot
pc_plt <- plotPCA(dat = ling_loc_quest, npcs = 2, y = kmeans_clust, 
                  is.discrete = T, var = T, center.pca = T, scale.pca = F, 
                  ylab = "Cluster", title = "", show.plt = F, 
                  alpha = .5, point.size = .1)$pca_plt +
  guides(color = F) +
  theme(axis.text = element_text(size = rel(.7)))

ggarrange(plotlist = list(counties_plt, pc_plt), nrow = 1, ncol = 2, 
          labels = NULL, widths = c(1.8, 1))

```


We next tried hierarchical clustering with various distance metrics and linkages, but for brevity, we only include the results using Euclidean distance with Ward's linkage. We note that from our exploration of hierarchical clustering which is not shown here, the clusters obtained from hierarchical clustering are \textit{heavily} dependent on the choice of linkage and distance metric, which is not a desirable property for stability. 

Similar to k-means, we choose to cut the tree by examining the elbow and silhouette plots for hierarchical clustering with Euclidean distance and Ward's linkage. Because of the similarities to the previous case with k-means, we omit the diagnostic plots and simply report that the silhouette plot showed an obvious peak at $k = 2$, and the elbow method agreed, so we choose to cut the tree to obtain $k = 2$ clusters. The results from hierarchical clustering with $k = 2$ clusters are shown on the US map in Figure \ref{fig:hierarchical_map} (We omit the hierarchical tree for brevity). We see that like k-means, hierarchical clustering also forms clusters that correspond to the northern and southern US. However, hierarchical clustering appears to be less stable and forms more heterogeneous clusters than k-means.

```{r hierarchical-choose-k, fig.width = 5.5, fig.height = 2.5, fig.align="center", fig.cap = "\\label{fig:hierarchical_choose_k} Metrics for choosing the number of clusters k in hierarchical clustering with Euclidean distance and Ward's linkage. From the within sum of squares and the silhouette information, it appears that k = 2 is an appropriate choice of k for the binary county-level linguistics data.", eval = F}

# run hierarchical clustering with ward's linkage and euclidean distance
# need to choose k via different 

# choose k via elbow method
elbow <- fviz_nbclust(x = ling_loc_quest,
                      FUNcluster = hcut, method = "wss",
                      hc_method = "ward.D", hc_metric = "euclidean") +
  labs(subtitle = "Elbow Method") +
  myGGplotTheme()

# choose k via silhouette method
sil <- fviz_nbclust(x = ling_loc_quest,
                    FUNcluster = hcut, method = "silhouette",
                    hc_method = "ward.D", hc_metric = "euclidean") +
  labs(subtitle = "Silhouette Method") +
  myGGplotTheme()


```


```{r hierarchical-tree, fig.width = 6, fig.height = 3, fig.align="center", fig.cap = "\\label{fig:hierarchical_tree} Here, we use hierarchical clustering with Ward's linkage and the Euclidean distance. The leaves of the tree are colored by latitude, where yellow corresponds to the eastern US, green the central US, and blue the western US."}
hclust_plt <- plotHclust(dat = ling_loc_quest, y = ling_loc$lat,
                         dist.metric = "euclidean", linkage = "ward.D")
# plot(hclust_plt, axes = F,
#      main = "Hierarchical Clustering")

```



```{r hierarchical-map, fig.width = 4, fig.height = 3, fig.align="center", fig.cap = "\\label{fig:hierarchical_map} Clusters given by hierarchical clustering with Ward's linkage and the Euclidean distance for k = 2, as shown on the US map. Note that the grey counties do not have any survey responses."}

hclust_cut <- as.factor(cutree(hclust_plt, k = 2))
plotCounties(county_df = ling_loc %>% select(county, state), 
             y = hclust_cut, ylab = "", 
             title = "Hierarchical Clustering", 
             show.plt = F, viridis_color = "viridis", county.line.color = NA) +
  guides(fill = F) +
  myGGplotFill(discrete = T, na.value = "lightgrey") +
  labs(subtitle = "(Ward's Linkage, Euclidean Distance)") + 
  theme(plot.title = element_text(face = "bold", size = rel(1), hjust = .5),
        plot.subtitle = element_text(size = rel(.9), hjust = .5))

```

Now, one drawback of k-means and hierarchical clustering is that we can no longer extract information about the features, which contribute to the patterns found by the clustering algorithm. Additionally, we can think of k-means and hierarchical clustering as "hard" clustering methods - an observation must be in one cluster or another; it cannot be in both. This is problematic in our linguistics analysis since both domain knowledge and the PC plots suggest a continuum of dialects, rather than completely disjoint groups.

We can overcome these two challenges using non-negative matrix factorization (NMF). As a matrix factorization method, NMF allows us to extract both sample patterns and feature patterns simultaneously. It can also be viewed as a "soft" clustering algorithm - that is, the weights in the NMF factors act as "probabilities" of the observation belonging to a certain cluster. Furthermore, NMF appears promising because the binary county-level linguistics data is entry-wise positive to begin with, so the non-negativity constraint will enforce sparsity and thus increase interpretability. For these reasons, we next try clustering the binary county-level linguistics data using NMF.

To choose the rank $k$ for NMF, we adopt a missing data imputation approach. The idea is to randomly leave out scattered missing elements of the data. Then, for each potential rank $k$, we apply NMF to the data with the missing values, and NMF outputs the two rank $k$ matrix factors $W$ and $H$. We impute the missing values via $X_{miss} = [WH]_{miss}$, and then we compute the imputation error (i.e. the difference between the imputed values and the observed data values). We repeat this process 20 times and choose $k$ according to the 1 standard error rule. As shown in Figure \ref{fig:nmf_choose_k}, $k = 4$ is the optimal rank for NMF according to the 1 standard error rule.

```{r nmf-choose-k-cv}
# try to choose k in nmf via missing data imputation
set.seed(123)
K_max <- 6 # max K to try
n_trials <- 20 # number of trials

err_mat <- matrix(NA, nrow = n_trials, ncol = K_max-1)
for (trial in 1:n_trials) {
  # randomly leave out scattered missing elements
  ind <- sample(nrow(ling_loc), size = .2*nrow(ling_loc), replace = F)
  dat <- as.matrix(ling_loc_quest)
  dat[ind] <- NA
  
  # impute the missing data points via NMF and compute imputation error
  err <- sapply(X = 2:K_max,
                FUN = function(k) {
                  z <- nnmf(dat, k)
                  W <- z$W
                  H <- z$H
                  reconstruct <- W %*% H # reconstruct X = WH
                  # compute imputation error
                  ans <- mean((reconstruct[ind] -
                                 as.matrix(ling_loc_quest)[ind])^2)
                  return(ans)
                })
  err_mat[trial,] <- err
}

# average the imputation error over the 20 trials
imputation_err <- colMeans(err_mat)
se <- apply(err_mat, 2, sd) / sqrt(n_trials) # compute se

```


```{r nmf-choose-k, fig.width = 2.5, fig.height = 2.25, fig.align="center", fig.cap = "\\label{fig:nmf_choose_k} We plot the mean imputation error +/- 1 standard error for various ranks of NMF (averaged over 20 trials). The dashed red line represents the threshold for the 1 standard error rule. Thus, the rank which satisfies the 1 standard error rule is 4."}

# plot imputation error
plt_df <- data.frame(k = 2:K_max, err = imputation_err, se = se)
min_se <- min(plt_df$err) + plt_df$se[which.min(plt_df$err)] # for 1 se rule
ggplot(plt_df) +
  aes(x = k, y = err) +
  geom_line(color = "black", size = .7) +
  geom_point(color = "black") +
  geom_errorbar(aes(ymin = err - se, ymax = err + se), width = .2,
                 position = position_dodge(.9), size = .25) +
  geom_hline(yintercept = min_se, linetype = "dashed", 
             col = "red", size = .7) +
  labs(x = "Rank k", y = "Imputation Error", title = "Choosing Rank for NMF") +
  myGGplotTheme(axis_title_size = rel(.7), axis_text_size = rel(.5), 
                title_size = rel(.8)) +
  theme(axis.line = element_line(size = .7))

```

Using $k = 4$, we next factorize the binary county-level linguistics data $X \in \mathbb{R}^{n \times p}$ into two rank 4 factors $W \in \mathbb{R}^{n \times 4}$ and $H \in \mathbb{R}^{4 \times p}$ via NMF. Since $W$ corresponds to the observation-level factors, we plot the weights of each column of $W$ on the US map to find clusters among the counties (see Figure \ref{fig:nmf}). On the other hand, since $H$ corresponds to the feature-level factors, features with the largest magnitudes in each row of $H$ correspond to the most important survey questions associated with the observed patterns in $W$. In Table 2, we list the questions which were found to be most important for each of the four clusters found my NMF.

```{r nmf, fig.width = 8, fig.height = 4, fig.align="center", fig.cap = "\\label{fig:nmf} In (A)-(D), we color each county by the loadings given in $W$. For each subplot, the counties with the largest magnitudes form a soft cluster."}

best_k_nmf <- 4

# do nmf
my_nmf <- nmf(ling_loc %>% select(starts_with("Q")), 
              rank = best_k_nmf, 'brunet')

# X = WH
w <- my_nmf@fit@W # n x k matrix
h <- my_nmf@fit@H # k x p matrix

# plot a map for each column of w
plt_w_df <- data.frame(w, ling_loc %>% select(-starts_with("Q"))) %>%
  filter(long > -125)
plt_ls <- list() # initialize list
for (k in 1:best_k_nmf) {
  plt_df <- plt_w_df
  colnames(plt_df)[k] <- "NMF Coeff"
  
  plt_ls[[k]] <- plotCounties(county_df = ling_loc %>% select(county, state), 
                              y = plt_df$`NMF Coeff`, viridis_color = "plasma",
                              ylab = "Loading", 
                              title = paste0("NMF Cluster ", k),
                              county.line.color = NA) +
    theme(legend.key.size = unit(.4, "cm"),
        legend.text = element_text(size = rel(.5)),
        plot.title = element_text(hjust = 0.5))
}

ggarrange(plotlist = plt_ls, nrow = ceil(best_k_nmf/2), ncol = 2)

```


```{r nmf-features, results = 'asis'}
# pull out the top features for each factor
top_max <- 3
top_qs <- apply(h, 1, FUN = function(X) {
  # find the ones with the largest magnitude
  top_names <- names(sort(X, decreasing = T))[1:top_max]
  
  # get indices of questions and answers
  q_num <- as.numeric(substr(top_names, start = 2, stop = 4))
  ans_num <- as.numeric(substr(top_names, 
                                 start = 6, stop = nchar(top_names)))
  keywords <- c(); idx <- 1
  for (q in q_num) {
    # find the keyword
    key <- paste0(all.ans[[q]]$ans[ans_num[idx]], " (Q", q, ")")
    keywords <- c(keywords, key); idx <- idx + 1
  }
  return(keywords)})

kable(top_qs, col.names = paste0("Cluster ", 1:4),
      caption = "Top Questions and Responses Corresponding to NMF Clusters")

```

After examining the clusters in Figure \ref{fig:nmf} and the corresponding features in Table 2, the four clusters from NMF are intuitive and reasonable. NMF appears to divide the US into four geographical regions - the south, northeast, mid west, and northwest, and the important features (i.e. question/responses) corresponding to the clusters also seems to match with intuition. For example, southerners are known for saying "y'all". Overall, this NMF analysis seems superior over k-means and hierarchical clustering. NMF allows us to capture a gradient between regions, rather than assigning hard clusters to the US. One could argue that this soft clustering technique enables NMF to select more clusters than what we have seen previously in k-means and hierarchical clustering. Lastly, the fact that we can extract meaningful patterns among counties \textit{and} the corresponding important questions/responses is a nice property of NMF.

\newpage

## Stability of findings to perturbation

One of the key findings above is that the clustering of dialects seems to be a continuum rather than a discrete clustering problem. We further investigate this finding by understanding whether or not the clusters from k-means (with $k = 2$) are stable. We consider instability from two sources: 1) being a local solution of a non-convex problem and 2) data perturbations.

We first checked stability of the k-means solution for different initializations. After running k-means 25 times, each time with a different initialization, we found that every run of k-means converged to one of three different solutions. The clusters given by these three solutions overlapped for all but 4 counties, and all three solutions gave a within sum of squares value equal to 25364.41. This indicates that k-means is stable with respect to different initializations.

```{r local-solution}
n_reps <- 25
k <- 2
diff <- c() # difference in clusters
ss <- c() # within sum of squares from kmeans
kmeans_base <- kmeans(x = top_pcs, centers = k) # baseline measure
kmeans_clust <- as.numeric(as.character(kmeans_base$cluster))
kmeans_clust_centers <- kmeans_base$centers
for (i in 1:n_reps) { # try kmeans for different starting points
  # which data points to initialize
  initial <- sample(1:nrow(ling_loc), k, replace = F) 
  kmeans_ans <- kmeans(x = top_pcs, centers = k)
  ss[i] <- kmeans_ans$tot.withinss
  clust <- kmeans_ans$cluster
  diff[i] <- min(sum(clust != kmeans_clust), sum(clust != -kmeans_clust + 3))
}

clust_diff <- unique(diff)
unique_ss <- unique(ss)

```

A perhaps more important source of instability comes from perturbations of the data. We next consider what happens to k-means when we randomly subsample $70\%$ of the data and apply k-means to this subsample. Note that we randomly subsample the county-level data to save on computation, but this can also be done using the original person-level survey data.

To make this stability comparison, we first run a baseline k-means on the full county-level dataset. Then for each of the $B = 200$ subsamples, we ran k-means on the subsampled data and reported the cluster labels on the subsampled data. We next counted the number of counties which were clustered differently in the subsample k-means than in the baseline k-means. 

```{r stability-subsample, fig.width = 4.5, fig.height = 1.5, fig.align = "center", fig.cap = "\\label{fig:subsample} We plot the distribution of the the number of counties which were clustered differently in the subsampled k-means compared to the full k-means. The outliers are plotted in red."}

set.seed(12345)
B <- 200
diff <- c(); centers_diff <- c()
ling_merged_ls <- list()
for (b in 1:B) {
  # subsample the people
  ind <- sample(1:nrow(ling_loc), .7*nrow(ling_loc), replace = F)
  ling_loc_samp <- data.frame(top_pcs[ind,], county = ling_loc$county[ind], 
                              state = ling_loc$state[ind])
  # do kmeans
  kmeans_ans <- kmeans(ling_loc_samp %>% 
                         select(starts_with("PC")), centers = 2)
  # kmeans_ans_ls[[b]] <- c(kmeans_ans, 
  #                         list(ling_loc_samp %>% select(county, state)))
  clust <- kmeans_ans$cluster
  clust_centers <- kmeans_ans$centers
  
  # compare difference between cluster labels
  clust_df <- data.frame(county = ling_loc_samp$county, 
                         state = ling_loc_samp$state, y_samp = clust)
  base_clust_df <- data.frame(county = ling_loc$county, 
                              state = ling_loc$state, y = kmeans_clust)
  ling_merged <- merge(x = clust_df, y = base_clust_df, 
                       by = c("county", "state"))
  ling_merged_ls[[b]] <- ling_merged
  
  # number of labels which are different between clusters
  diff[b] <- min(sum(ling_merged$y_samp != ling_merged$y), 
                 sum(ling_merged$y_samp != -ling_merged$y + 3))
  # change in the centroid distances
  centers_diff[b] <- min(norm(clust_centers - kmeans_clust_centers, "F")^2,
                         norm(clust_centers - 
                                kmeans_clust_centers[2:1,], "F")^2)
}

plt_df <- data.frame(diff = diff)
plt_outlier_df <- data.frame(outlier = 
                               diff[diff > median(diff) + IQR(diff) * 1.5])

ggplot(plt_df) +
  aes(x = "", y = diff) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(aes(y = outlier, x = ""), data = plt_outlier_df, size = .5,
              color = "red", width = .075, height = 0) +
  labs(y = "Number of Counties that Switched Clusters") +
  coord_flip() +
  myGGplotTheme(axis_text_size =  rel(.7), axis_title_size = rel(.8))

```

We plot the distribution of these differences in Figure \ref{fig:subsample}, and we see that there are numerous instances where the clusters differed by more than 100 counties. From this, we looked more closely into a case where the clusters differed by 200 counties and plotted this in Figure \ref{fig:stability_map}. We notice that k-means had difficulties clustering the counties near the northeast. Overall, from this preliminary stability analysis, we see that k-means can be variable and yield different clusters when the data is perturbed, but it is not too severe. We hypothesize that if we had taken $k = 3$, then the stability of k-means would be much worse.


```{r stability-map, fig.width = 4.5, fig.height = 3, fig.align="center", fig.cap = "\\label{fig:stability_map} In this map, we colored the 200 counties, which were clustered differently by the full k-means and an instance of the subsampled k-means algorithm.", fig.pos = "H"}

# plot map of counties that flipped for case when diff ~ 200
tmp <- ling_merged_ls[[which.max(diff[diff < 600])]]
if (sum(tmp$y_samp != tmp$y) < sum(tmp$y_samp != -tmp$y + 3)) {
  diff_county <- tmp[which(tmp$y_samp != tmp$y),]
}else {
  diff_county <- tmp[-which(tmp$y_samp != tmp$y),]
}

plotCounties(county_df = diff_county %>% select(county, state),
             y = as.factor(rep(1, nrow(diff_county))), 
             ylab = "", 
             title = "Counties which Changed Clusters") +
  guides(fill = F) +
  theme(plot.title = element_text(hjust = 0.5))

```


## Conclusion

From our analysis of the county-level linguistics data and the success of soft clustering via NMF, we conclude that linguistic variation should be viewed as a continuum over space, rather than disparate regional identities. NMF was also useful from an interpretation standpoint and gave meaningful feature patterns which intuitively meshed with the county clusters. Lastly, stability of NMF can likely be improved with something like staNMF, but in this report, we looked into the stability of k-means with $k = 2$ and found that it was mostly stable but can drastically change in certain situations. 


# Bibliography
